{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental simple performance testing notebook for Spark\n",
    "- testing and comparing simple dataframe / sql operations of commong data (pre-)processing tasks \n",
    "- various available single-machine Python solutions are to be tested: Pandas, PySpark, Turi Create and Dask.\n",
    "- execution times, CPU load and maximal memory use should be tracked\n",
    "\n",
    "\n",
    "## Kiva dataset \n",
    "- [Kiva](https://www.kaggle.com/gaborfodor/additional-kiva-snapshot): crowdfunding data with lenders and loans, with additional geographic data\n",
    "- download the related CSV files and move them to a folder where the kernel can read them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "import timeit \n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# use SparkSession instead of SparkContext: \n",
    "# setting SparkSession config paramters are necesary to use available memory (we can also limit CPUs by eg. \n",
    "# .config('spark.default.parallelism', 5), but it uses all the CPUs by default)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('PySpark local test') \\\n",
    "    .config(\"spark.core.connection.ack.wait.timeout\", \"12000\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config('spark.executor.memory', '4G') \\\n",
    "    .config('spark.driver.memory', '5G') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read files to dataframes: loans and lenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_start = timeit.default_timer()\n",
    "\n",
    "lenders_df = spark.read.csv(\"../../kiva/lenders.csv\", header=True)  # 130 MB file, 2.349.174 lines\n",
    "lenders_df.createOrReplaceTempView(\"lenders\") \n",
    "loans_df = spark.read.csv(\"../../kiva/loans.csv\", header=True)      # 2.1 GB file, 1.419.607 lines\n",
    "loans_df.createOrReplaceTempView(\"loans\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read, transform and count loan_lenders \n",
    "string enumeration to rows: split tuple strings to array, then explode the array to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lldf = spark.read.csv(\"../../kiva/loans_lenders.csv\", header=True) # .limit(20) \n",
    "\n",
    "loans_lenders_df = lldf.select( \\\n",
    "      lldf.loan_id, explode(split(lldf.lenders, ', ?')).alias('lender') \\\n",
    ").distinct() \n",
    "\n",
    "loans_lenders_df.createOrReplaceTempView(\"loans_lenders\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join, filter and sort loan and lender data\n",
    "get distinct joined lines with renamed columns, then write to an output file (for fully materialized results)\n",
    "- filtering on lenders.country_code: \n",
    "  - 'US': 25% of lenders\n",
    "  - 'CA': 3% of lenders --> 3.5 GB joined, 1.971.548 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join and filter using SQL: \n",
    "joined_df = spark.sql(\"\"\"\n",
    "select distinct \n",
    "  le.permanent_name as lender_permanent_name, le.display_name as lender_display_name, \n",
    "  le.city as lender_city, le.state as lender_state, le.country_code as lender_country_code, \n",
    "  le.member_since as lender_member_since, le.occupation as lender_occupation, \n",
    "  le.loan_because as lender_loan_because, le.loan_purchase_num as lender_loan_purchase_num, \n",
    "  le.invited_by as lender_invited_by, le.num_invited as lender_num_invited, \n",
    "  lo.loan_id, lo.loan_name, lo.original_language, lo.description, lo.description_translated, \n",
    "  lo.funded_amount, lo.loan_amount, lo.status, lo.activity_name, lo.sector_name, \n",
    "  lo.loan_use, lo.country_code, lo.country_name, lo.town_name, lo.currency_policy, \n",
    "  lo.currency_exchange_coverage_rate, lo.currency, lo.partner_id, lo.posted_time, \n",
    "  lo.planned_expiration_time, lo.disburse_time, lo.raised_time, lo.lender_term, \n",
    "  lo.num_lenders_total, lo.num_journal_entries, lo.num_bulk_entries, lo.tags, \n",
    "  lo.borrower_genders, lo.borrower_pictured, lo.repayment_interval, lo.distribution_model\n",
    "from   loans_lenders as ll\n",
    "         inner join loans lo ON lo.loan_id = ll.loan_id\n",
    "         inner join lenders le ON le.permanent_name = ll.lender\n",
    "where  le.country_code = 'CA'\n",
    "order by lender_permanent_name, loan_id\n",
    "\"\"\")\n",
    "\n",
    "joined_df.createOrReplaceTempView(\"joined\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group and sort joined data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* group by on the exploded loans_lenders table (6 GB): count distinct loan_id by lender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full ellapsed time:  359.76395748999494\n"
     ]
    }
   ],
   "source": [
    "# remove previous results if exists: \n",
    "import shutil\n",
    "try: \n",
    "    shutil.rmtree('../../kiva/pyspark-result-groupby.csv')\n",
    "except FileNotFoundError: \n",
    "    pass\n",
    "\n",
    "lender_loan_count_df = spark.sql(\"\"\"\n",
    "select lender_permanent_name, count(distinct loan_id) as loan_ct\n",
    "from   joined \n",
    "group by lender_permanent_name\n",
    "-- order by count(distinct loan_id) desc\n",
    "\"\"\")\n",
    "\n",
    "lender_loan_count_df.createOrReplaceTempView(\"lender_loan_count\")\n",
    "\n",
    "lender_loan_count_df.coalesce(1).write.csv('../../kiva/pyspark-result-groupby.csv', header=True)\n",
    "\n",
    "print('full ellapsed time: ', timeit.default_timer() - full_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
